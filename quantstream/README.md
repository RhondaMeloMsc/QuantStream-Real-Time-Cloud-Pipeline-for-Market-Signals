
# ğŸ“¡ QuantStream â€” Real-Time Cloud Pipeline for Market Signals

**Serverless reference architecture** for streaming crypto + equity market data into a cloud pipeline with **ingestion â†’ normalization â†’ anomaly detection â†’ alerts**.  
Designed to demonstrate **AWS cloud architecture** while remaining fully **runnable locally** for demos.

> Tech focus: **AWS Lambda, Kinesis, DynamoDB, EventBridge** (+ local simulation), **Python**, **ML anomaly flags**.

---

## ğŸŒ What this shows
- **Event-driven design** for market data
- **Cloud resources** (via AWS SAM template) â€” Kinesis stream, DynamoDB tables, scheduled Lambda
- **ML-lite** anomaly detection (rolling z-score) for liquidity shocks & spikes
- **Local runner** (no AWS needed) to fetch data (CoinGecko), normalize, flag anomalies, and store CSV artifacts

---

## ğŸ—‚ Repository layout
```
quantstream/
â”œâ”€ src/
â”‚  â”œâ”€ lambdas/
â”‚  â”‚  â”œâ”€ ingest_coingecko.py     # (Lambda) poll CoinGecko â†’ push to stream
â”‚  â”‚  â”œâ”€ enrich_normalize.py     # (Lambda) normalize stream payloads
â”‚  â”‚  â””â”€ anomaly_detect.py       # (Lambda) z-score flags â†’ Alerts table
â”‚  â””â”€ shared/
â”‚     â””â”€ utils.py                # common parsing/math
â”œâ”€ infra/
â”‚  â””â”€ template.yaml              # AWS SAM: Kinesis + DynamoDB + Lambdas + schedule
â”œâ”€ local/
â”‚  â”œâ”€ run_local.py               # Local end-to-end simulation (no AWS required)
â”‚  â””â”€ sample_config.yaml         # Assets, thresholds, intervals
â”œâ”€ tests/
â”‚  â””â”€ test_anomaly.py            # minimal unit test for z-score logic
â”œâ”€ data/.gitkeep
â”œâ”€ artifacts/.gitkeep
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â””â”€ README.md
```

---

## ğŸ› ï¸ Local quickstart (no AWS required)

```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
python local/run_local.py --config local/sample_config.yaml --cycles 5
```

This will:
1. Fetch prices for configured assets
2. Normalize + compute rolling z-scores
3. Write outputs to `artifacts/` (CSV) and print any anomaly alerts

> You can increase `--cycles` to keep streaming locally. Use Ctrl+C to stop.

---

## â˜ï¸ AWS deployment (reference)

The repo includes an **AWS SAM** template (`infra/template.yaml`) wiring:
- **Kinesis**: `MarketStream`
- **DynamoDB**: `PricesTable`, `AlertsTable`
- **EventBridge**: Scheduler for `IngestFunction` (e.g., every minute)
- **Three Lambdas**: Ingest â†’ Normalize â†’ Anomaly

> **Note:** This template is a reference for interviews/architecture reviews. You can deploy with `sam build && sam deploy` after adding your AWS account/region, but the local runner provides a fast, zero-cost demo.

---

## ğŸ§ª Anomaly detection (how it works)
We compute **rolling mean/std** per asset and flag any point where `|z| > threshold`.  
This is intentionally simple; plug in your own model (e.g., Isolation Forest, Prophet) inside `src/lambdas/anomaly_detect.py` or the local runner.

---

## ğŸ” Notes
- This is educational demo code â€” **not** investment advice.
- Keep secrets out of git. Use `.env` only locally.

MIT Â© 2025 Rhonda Melo
